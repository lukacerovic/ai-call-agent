# ğŸ“ Test: Full Voice Conversation Flow

**New feature: Frontend now properly captures and sends voice!**

This guide explains the exact conversation flow you should see.

---

## ğŸš€ Quick Start (3 Minutes)

### **Step 1: Pull Latest Code**

```bash
cd ai-call-agent
git pull origin main
```

### **Step 2: Restart Backend**

```bash
cd backend
# Press Ctrl+C to stop

uvicorn main:app --reload
```

### **Step 3: Restart Frontend**

```bash
cd frontend
# Press Ctrl+C to stop

npm start
```

### **Step 4: Open Browser DevTools**

```
http://localhost:3000
Press F12
Go to: Console tab
```

You'll see LOTS of debug messages showing the voice flow.

---

## ğŸ† Expected Conversation Flow

### **1. You Click â˜ï¸ Call Clinic**

**Frontend Console Shows:**
```
âœ… Microphone access granted
ğŸ“¡ Connecting to WebSocket: ws://localhost:8000/ws
âœ… WebSocket connected
ğŸ¤ Starting voice capture...
```

**Backend Shows:**
```
ğŸ“ New call connected
ğŸ¤ Agent: Hello, thank you for calling Local Medical Clinic...
ğŸ“¤ Sending greeting audio: 5234 bytes
ğŸ§ Waiting for user audio...
```

**You Hear:** AI greeting about clinic services

**Status:** "Listening..."

---

### **2. You Start Speaking**

Example: *"I am having problems with my heart and I need to check it fast. Can you suggest which service should I take?"*

**Frontend Console Shows:**
```
ğŸ”´ Speech detected, starting recording
```

**Status:** Changes to "Listening..."

**What's Happening:** 
- Microphone captures your voice
- MediaRecorder records audio chunks
- Frontend analyzes frequency to detect speech

---

### **3. You Stop Speaking (Silence for 2-3 seconds)**

**Frontend Console Shows:**
```
ğŸ”‡ Silence detected (1/15)
ğŸ”‡ Silence detected (2/15)
ğŸ”‡ Silence detected (3/15)
...
ğŸ”‡ Silence detected (15/15)
â¹ï¸ Silence duration reached, stopping recording and sending
ğŸ“ Audio chunk received: 4096 bytes
ğŸ“¦ Compiled audio blob: 48234 bytes
ğŸ“¤ Sending audio to backend: 48234 bytes
```

**What's Happening:**
- Frontend detects 1.5 seconds of silence
- Stops recording your voice
- Converts WebM audio to blob
- Sends via WebSocket to backend
- Shows: "[Processing your speech...]"

---

### **4. Backend Receives and Processes**

**Backend Shows:**
```
ğŸ“¥ Receiving from WebSocket...
ğŸ“Š Received audio chunk: 48234 bytes
ğŸ” Running Voice Activity Detection...
ğŸ” VAD result: True (speech detected)
ğŸ¤ Starting Speech-to-Text...
ğŸ“ Transcription result: 'I am having problems with my heart...'
ğŸ‘¤ User: I am having problems with my heart and I need to check it fast. Can you suggest which service should I take?
```

**What's Happening:**
- Backend receives audio bytes
- Voice Activity Detection confirms speech
- Google Speech Recognition transcribes to text
- Text sent to Ollama AI agent

---

### **5. AI Agent Processes and Responds**

**Backend Shows:**
```
ğŸ§  Sending to agent: 'I am having problems with my heart...'
ğŸ¤ Agent: I understand you're experiencing heart-related concerns. For heart issues, I recommend our Cardiology service. Let me help you schedule an appointment with one of our cardiologists. What date and time would work best for you?
ğŸ”Š Converting response to speech...
ğŸ“¤ Sending response audio: 8934 bytes
âœ… Message #1 completed
```

**What's Happening:**
- Ollama LLM processes your message
- Generates natural response
- pyttsx3 converts to speech (offline)
- Sends audio back to frontend

---

### **6. You Hear AI Response**

**Frontend Console Shows:**
```
ğŸ“¥ Received from backend: 8934 bytes
ğŸ”Š Playing audio: 8934 bytes
â–¶ï¸ Audio started playing
```

**You Hear:** *"I understand you're experiencing heart-related concerns..."*

**Status:** Changes to "Agent Speaking..."

---

### **7. AI Finishes Speaking, Loop Repeats**

**Frontend Console Shows:**
```
â¹ï¸ Audio finished playing
ğŸ¤ Starting voice capture...
```

**Status:** Changes back to "Listening..."

**What's Happening:**
- After AI finishes speaking
- Frontend automatically starts listening again
- Ready for your next message

---

### **8. You Respond to Agent**

Example: *"I prefer next Tuesday at 2 PM"*

**The loop repeats:** Speech detection â†’ Silence â†’ Send â†’ Process â†’ Response â†’ Listen

---

### **9. You End Call**

Click **ğŸ“µ End Call**

**Frontend Console Shows:**
```
ğŸ“µ Ending call
ğŸ›‘ Stopping audio capture
ğŸ”Œ WebSocket disconnected
```

**Backend Shows:**
```
ğŸ“ Call disconnected
ğŸ“Š Call session session-1234567890.123456 ended (messages processed: 3)
```

---

## ğŸ› Console Debug Messages

### Frontend (Browser Console - F12)

**Good signs:**
```
âœ… Microphone access granted
âœ… WebSocket connected
ğŸ”´ Speech detected, starting recording
â¹ï¸ Silence duration reached, stopping recording and sending
ğŸ“¤ Sending audio to backend: X bytes
ğŸ”Š Playing audio: X bytes
```

**Bad signs:**
```
âŒ Microphone access denied
âŒ WebSocket error
âŒ Failed to play audio
```

### Backend (Terminal Output)

**Good signs:**
```
ğŸ“¥ Receiving from WebSocket...
ğŸ“Š Received audio chunk: X bytes
ğŸ” VAD result: True
ğŸ“ Transcription result: 'user text'
ğŸ‘¤ User: user text
ğŸ¤ Agent: response text
ğŸ“¤ Sending response audio: X bytes
```

**Bad signs:**
```
âŒ No "Received audio chunk" messages
ğŸ” VAD result: False (microphone too quiet)
ğŸ“ Transcription result: '' (empty - bad audio quality)
```

---

## ğŸ”Š How Voice Detection Works

### **Speech Detection (Frontend)**

```
Microphone Audio Stream
        â†“
   Analyze Frequency
        â†“
   Is Average > 25? (Threshold)
        â†“
    YES: Recording ON ğŸ”´
    NO:  Silence Counter++
        â†“
   Silence Counter >= 15? (1.5 seconds)
        â†“
    YES: Stop Recording & Send ğŸ“¤
    NO:  Continue Listening
```

### **Why 2-3 Seconds Silence?**

- **Natural speech pattern:** People naturally pause between thoughts
- **Prevents accidental sending:** Short pauses within sentences don't trigger send
- **Detects end of message:** 1.5 second pause = confident message is complete

**Example:**
- "I have... [0.3s pause]... heart problems" â†’ Still recording (pause < 1.5s)
- "I have heart problems" â†’ [2 second silence] â†’ Send! (silence >= 1.5s)

---

## ğŸ”¡ How STT (Speech-to-Text) Works

### **Flow:**

```
Audio Blob (WebM format)
        â†“
   Backend Receives
        â†“
   Voice Activity Detection (VAD)
   - Is this actual speech? YES âœ…
        â†“
   Google Speech Recognition
   - Convert audio to text
   - "I have heart problems"
        â†“
   Ollama AI Agent
   - Process message
   - Generate response
        â†“
   pyttsx3 Text-to-Speech
   - Convert response to audio
        â†“
   Send Audio Back to Frontend
```

---

## âœ… Checklist: Voice Conversation Working?

```
[ ] Click â˜ï¸ Call Clinic
[ ] Hear AI greeting
[ ] Status shows "Listening..."
[ ] Speak into microphone
[ ] Frontend console shows: "ğŸ”´ Speech detected, starting recording"
[ ] Pause for 2-3 seconds
[ ] Frontend console shows: "â¹ï¸ Silence duration reached, stopping recording and sending"
[ ] Backend console shows: "ğŸ“Š Received audio chunk"
[ ] Backend console shows: "ğŸ“ Transcription result: '[your speech]'"
[ ] Backend console shows: "ğŸ‘¤ User: [your speech]"
[ ] Backend console shows: "ğŸ¤ Agent: [response]"
[ ] Backend console shows: "ğŸ“¤ Sending response audio"
[ ] You hear AI response
[ ] Status changes back to "Listening..."
[ ] Speak again - conversation continues
[ ] Click ğŸ“µ End Call to disconnect
```

**All checked? Voice system is working!** ğŸ‰

---

## ğŸ› Troubleshooting

### Problem: Frontend doesn't show "Speech detected" message

**Cause:** Microphone not capturing audio or threshold too high

**Fix:**
- Speak LOUDER
- Get mic CLOSER to mouth
- Reduce background noise
- Check Windows Volume Mixer

### Problem: Backend shows "VAD result: False"

**Cause:** Audio too quiet for voice detection

**Fix:** Same as above - speak louder

### Problem: Backend shows empty transcription

**Cause:** Google Speech Recognition couldn't understand

**Fix:**
- Speak more clearly
- Reduce background noise
- Check internet connection (Google STT needs it)

### Problem: No audio response from AI

**Cause:** Either:
1. Ollama not responding
2. TTS conversion failed
3. WebSocket not open

**Fix:**
- Check Ollama: `curl http://localhost:11434/api/tags`
- Check backend logs for errors
- Restart backend

---

## ğŸ¯ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          FRONTEND (React)             â”‚
â”‚       http://localhost:3000          â”‚
â”‚                                      â”‚
â”‚   1. Capture Microphone Audio       â”‚
â”‚   2. Detect Speech vs Silence       â”‚
â”‚   3. Convert to Audio Blob          â”‚
â”‚   4. Send via WebSocket             â”‚
â”‚   5. Play Response Audio            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ WebSocket /ws (Real-time Audio)
         â”‚ Bi-directional, encrypted
         â”‚
         v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         BACKEND (FastAPI)           â”‚
â”‚       http://localhost:8000         â”‚
â”‚                                      â”‚
â”‚   1. Receive Audio Bytes            â”‚
â”‚   2. Voice Activity Detection       â”‚
â”‚   3. Speech-to-Text (Google)       â”‚
â”‚   4. Send to Ollama AI              â”‚
â”‚   5. Text-to-Speech (pyttsx3)      â”‚
â”‚   6. Send Response Audio            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ HTTP (Ollama Integration)
         â”‚
         v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          OLLAMA (Local AI)          â”‚
â”‚       http://localhost:11434        â”‚
â”‚                                      â”‚
â”‚   LLM: llama2                        â”‚
â”‚   - Understands patient needs      â”‚
â”‚   - Generates intelligent responses â”‚
â”‚   - Offline (private)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Ready to Test?

1. **Get latest code**: `git pull origin main`
2. **Restart backend**: `uvicorn main:app --reload`
3. **Restart frontend**: `npm start`
4. **Open DevTools**: F12 â†’ Console
5. **Call clinic**: Click button
6. **Speak naturally**: Say something about your health
7. **Pause for 2-3 seconds**: Signals end of message
8. **Listen for response**: AI replies naturally
9. **Continue conversation**: Loop repeats

**Your voice conversation system is now complete!** ğŸ‰
